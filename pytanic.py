# TITANIC PROJECT IN PYTHON: pytanic
import os
import shutil
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# custom libs
import selector

# prepare repo
if not os.path.exists("predictions"):
    os.mkdir("predictions")
# create img folder
if os.path.exists("img"):
    shutil.rmtree("img")
    os.mkdir("img")
# prepare data
# load the training and test data
train = pd.read_csv('./data/train.csv', index_col='PassengerId')
test = pd.read_csv('./data/test.csv', index_col='PassengerId')
# leaked target
y_test = pd.read_csv("predictions/y_leaked.csv", index_col="PassengerId")

# missing values count
print("Missing values count")
print(selector.show_missing_values(train, test))
# marginal distributions and pairwise relationships between the features
# it is marginal because it describes the behavior of a specific variable without keeping the others fixed.
#g = sns.pairplot(train, hue="Survived", kind='reg', diag_kind='kde')
#g.savefig('img/marginal_distribution', bbox_inches='tight')
# decribe features
#train.describe(include="all")
# the followings apply to the datasets:
# Surived increase when Fare increases (+)
# Age decreases when SibSp increases (-)
# Fare distribution reveals that it has a long tail. For this reason, we should take its logarithm to turn it approximately into a normal distribution (linear models work best for normal error distribution).
# SibSp and Parch have also a long tail but they are discrete values. We will consider them as categorical, so we encode them and let a given model decide on the importance of the levels. However, we merge them to reduce the total number of categorical levels generated by the encoding.

# Name, Cabin, and Ticket features are not usable in their current state. We should use them to engineer new features and exclude them from training.

# Name is an identifier as it is unique for each passenger.
# the Title instead has smaller cardinality but it is also related to the gender and this might cause collinearity. Some rare titles should be grouped to deduce dimensions

# Cabin tells the location of a passenger on the Titanic. A closer location to the lifeboats might affect survival. Cabin has many missing values therefore we create 2 features:
#  * Cabin_ind: whether a passenger has Cabin info or not
#  * Cabin_deck: the deck on which the cabin is located
# For more info, see https://en.wikipedia.org/wiki/Titanic#Dimensions_and_layout

# EXPERIMENTS
# fix wrong features types
cat_cols = ["Name", "Sex", "Ticket", "Cabin", "Embarked"]
for temp in [train, test]:
    temp[cat_cols] = temp[cat_cols].astype("category")
# Pclass categorical feature can be considered either nominal or ordinal. For simplicity we consider it ordinal so we keep the number of features low.

# the learning curve shows that the classifier is overfitting. This conclusion is drawn by the large variability (or errors) of the cross-validation score. Most likely, the classifier could benefit to reduce overfitting from more data points given the uptrend direction of both training and cross-validation curves. However, this is not possible in this case, therefore we need another way to help the model generalize better. We could reduce the number of features by means of the followings:
#  - remove statistically insignificant features or features correlated with other features and not with target
#  - engineer new features from the existing one
#  - add Features Selection and/or Dimensionality Reduction in the pipeline

# remove statistically insignificant features or features correlated with other features and not with target

# when all other features remain constant, i.e., conditional dependencies, negative coefficients such as Sex_male, Pclass_3, SibSp, and Age are likely associated with the death of a passenger. Whereas Fare is associated with the survival of a passenger. However, looking at the coefficient plot to gauge feature importance can be misleading as some of them vary on a small scale, while others, like Age, varies a lot more, several decades. This is visible if we compare their standard deviations. Pclass_3 is logically correlated with Pclass_2 as they were created from the same feature. However, Pclass_3 is also correlated with Fare and this may add confusion in the model. We plot the features dependence to confirm the correlations among features and find if we can get rid of misleading features. Pclass_3 can be (100%) predicted from Fare so we should remove Pclass and still obtain equivalent results

# engineer new features from the existing one
# SipSp and Parch could be merged together into a FamilySize feature. Even if the CV score does not improve much, we still obtained a simpler model that we can tweak further.


# FEATURES ENGINEERING

# TITLE
# title mapping
title_map = {'ms': 'mrs', 'mme': 'mrs', 'mlle': 'miss'}
title_map.update({title: 'rare' for title in ['dr', 'rev', 'sir', 'don', 'jonkheer', 'lady', 'the countess', 'col', 'major', 'capt', 'dona']})
# the Title of the passenger indicates gender, age, and social class
def parse_title(name):
    # title is between last and first names
    title = name.split(',')[1].lstrip().split('.')[0].lower()
    return title_map.get(title, title)

# TICKET
# we do not have metadata for Ticket so it is hard to make a valid hypothesis to engineer a feature out of it

# BINNED
from sklearn.linear_model import LogisticRegressionCV
f = ["Age", "Fare", "SibSp", "Parch", "Pclass", "Sex", "Embarked"]
y_train = train["Survived"]
X_train = train[f]
X_test = test[f]
# create pipeline
pipe, pipe_res, _ = selector.init_pipeline(X_train, y_train, cv=5)
# transform training dataset
train_tr, transf, enc_cols = selector.transform_dataset(pipe, X_train, y_train)
test_tr = pd.DataFrame(transf.transform(X_test), columns=enc_cols)
# evaluate binning
reports = []
f = ["Age", "Fare"]
Cs = np.logspace(-4, 2, 50)
f_med = train[f].median()
f_train = train[f].fillna(f_med)
f_test = test[f].fillna(f_med)
print("Evaluate binning of", f)
for b in range(2, 11):
    # train bin
    f_bin_train = pd.get_dummies(pd.concat([pd.cut(f_train[c], bins=b, labels=[f"{c}_{i}" for i in range(b)]) for c in f], axis=1)).reset_index(drop=True)
    # test bin
    f_bin_test = pd.get_dummies(pd.concat([pd.cut(f_test[c], bins=b, labels=[f"{c}_{i}" for i in range(b)]) for c in f], axis=1)).reset_index(drop=True)
    # concatenated with transformed
    X_bin_train = pd.concat([train_tr.drop(columns=f), f_bin_train], axis=1)
    X_bin_test = pd.concat([test_tr.drop(columns=f), f_bin_test], axis=1)
    # get pipeline classifier
    model = pipe["classifier"].fit(X_bin_train, y_train)
    # evaluate
    report = {
        "nbin": b,
        "test_score": model.score(X_bin_test, y_test),
        "train_score": model.score(X_bin_train, y_train)
    }
    reports.append(report)
# assess
df = pd.DataFrame(reports)
print(df.sort_values("test_score", ascending=False))

"""
Binning helps but very little.
   bin     score
3    5  0.779904
7    9  0.775120
0    2  0.770335
2    4  0.770335
4    6  0.770335
6    8  0.765550
5    7  0.763158
1    3  0.760766
8   10  0.758373
"""

# FAMILYSIZE
# CABININD
# ISALONE
# TITLE
for df in [train, test]:
    # SibSp+Parch become FamilySize feature
    df["FamilySize"] = 1 + df["SibSp"] + df["Parch"]
    # whether a passenger has Cabin info or not
    df["CabinInd"] = df["Cabin"].notna()
    # whether the passenger travels alone or not
    df["IsAlone"] = df['FamilySize'] == 1
    # passenger's title
    df["Title"] = df["Name"].apply(parse_title).astype("category")

# define hypotheses
random_state = 42
cv = 5
# extract target
y_train = train["Survived"]
# define hypotheses with predictors
h1 = ["Age", "Fare", "SibSp", "Parch", "Pclass", "Sex", "Embarked"]
h2 = ["Age", "Fare", "SibSp", "Parch", "Pclass", "Sex", "Embarked", "FamilySize", "CabinInd", "IsAlone", "Title"]
h3 = ["Age", "Pclass", "Sex", "FamilySize", "CabinInd", "IsAlone", "Title"]
h4 = ["Age", "Fare", "SibSp", "Parch", "Sex", "FamilySize", "CabinInd", "IsAlone"]
# run hypotheses
for index, hypothesis in enumerate([h1, h2, h3, h4]):
    # setup
    train_h = train[hypothesis]
    test_h = test[hypothesis]
    # tag for plot classification
    info = f"H_{index}"
    print(info, "training shapes:", train_h.shape, y_train.shape)
    print("predictors:", hypothesis)
    # create pipeline
    pipe, pipe_res, _ = selector.init_pipeline(train_h, y_train, cv=cv)
    print("best_params:", _)
    # evaluate pipeline against the (hypothesis) training dataset
    selector.plot_learning_curves(pipe, train_h, y_train, cv=cv, info=info)
    # transform training dataset
    train_h_tr, transf, enc_cols = selector.transform_dataset(
        pipe, train_h, y_train
    )
    # evaluate hypothesis via coefficients importance and variability
    selector.plot_coefficient_importance(train_h_tr, y_train, cv=cv, info=info)
    # features Pearson correlation
    selector.plot_features_correlation(train_h_tr, info=info)
    # features collinearity, i.e. dependence with other features than the target
    selector.plot_features_dependence(train_h_tr, info=info)
    # test on leaked dataset
    test_h_tr = pd.DataFrame(transf.transform(test_h), columns=enc_cols)
    selector.accuracy(
        pipe["classifier"], train_h_tr, y_train, test_h_tr, y_test
    )
# END EXPERIMENTS
