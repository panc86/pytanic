# TITANIC PROJECT IN PYTHON: pytanic
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# custom libs
import pipelines
from selection import (
    show_missing_values, get_encoded_features,
    plot_coefficients, plot_learning_curves
)

# prepare data
# load the training and test data
train = pd.read_csv('./data/train.csv', index_col='PassengerId')
test = pd.read_csv('./data/test.csv', index_col='PassengerId')
# training info
train.info()
# fix wrong features types
cols_to_fix = ['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']
for df in [train, test]:
    df[cols_to_fix] = df[cols_to_fix].astype("category")
# marginal distributions and pairwise relationships between the features
# it is marginal because it describes the behavior of a specific variable without keeping the others fixed.
g = sns.pairplot(train, hue="Survived", kind='reg', diag_kind='kde')
g.savefig('img/marginal_dist', bbox_inches='tight')
# decribe features
train.describe(include="all")
# the followings apply to the datasets:
# Surived increase when Fare increases (+)
# Age decreases when SibSp increases (-)
# Fare distribution reveals that it has a long tail. For this reason, we should take its logarithm to turn it approximately into a normal distribution (linear models work best for normal error distribution).
# SibSp and Parch have also a long tail but they are discrete values. We will consider them as categorical, so we encode them and let a given model decide on the importance of the levels. However, we merge them to reduce the total number of categorical levels generated by the encoding.

# Name, Cabin, and Ticket features are not usable in their current state. We should use them to engineer new features and exclude them from training.

# Name is an identifier as it is unique for each passenger.
# the Title instead has smaller cardinality but it is also related to the gender and this might cause collinearity. Some rare titles should be grouped to deduce dimensions

# Cabin tells the location of a passenger on the Titanic. A closer location to the lifeboats might affect survival. Cabin has many missing values therefore we create 2 features:
#  * Cabin_ind: whether a passenger has Cabin info or not
#  * Cabin_deck: the deck on which the cabin is located
# For more info, see https://en.wikipedia.org/wiki/Titanic#Dimensions_and_layout

from sklearn.model_selection import GridSearchCV

# EXPERIMENTS

# keep reproducibility
random_state = 42
# set target name
target = "Survived"

# EXPERIMENT 1
numeric_features = ["Age", "Fare", "SibSp", "Parch"]
categorical_features = ["Pclass", "Sex", "Embarked"]
# extract target and predictors
predictors = [*numeric_features, *categorical_features]
y_train = train[target]
X_train = train[predictors]
X_test = test[predictors]
# what predictors have missing values?
show_missing_values(X_train, X_test)

# p0 pipeline:
#  - impute missing values: Age, Fare, and Embarked
#  - standardize numeric_features
#  - onehot encode categorical_features
# build pipeline
p0, params = pipelines.baseline_pipeline(numeric_features, categorical_features)
# grid search best hyperparameters config
p0_gs = GridSearchCV(p0, params, verbose=1, n_jobs=-1).fit(X_train, y_train)
print("score:", p0_gs.best_score_)
print("params:", p0_gs.best_params_)
# how is the model learning?
plot_learning_curves(
    p0.set_params(**p0_gs.best_params_), X_train, y_train, info="p0"
)
# print features importance
# fit p0 pipeline
_ = p0.fit(X_train, y_train)
# transform data
p0_transformer = p0["transformer"]
onehot_features = get_encoded_features(p0_transformer, categorical_features)
transformed_features = [*numeric_features, *onehot_features]
# training
X_train_tr = pd.DataFrame(
    p0_transformer.transform(X_train), columns=transformed_features
)
# testing
X_test_tr = pd.DataFrame(
    p0_transformer.transform(X_test), columns=transformed_features
)
# what predictors have missing values?
show_missing_values(X_train_tr, X_test_tr)
# how is the importance of features?
plot_coefficients(p0, X_train_tr)
# positive coefficients such as Age_0 and Fare are likely
# associated with the survival of a passenger. Whereas negative
# coefficients are associated with the death of a passenger.
# However, looking at the coefficient plot to gauge feature
# importance can be misleading as some of them vary on a small scale,
# while others, like Age, varies a lot more, several decades.
# This is visible if we compare their standard deviations.

